documentationURL: "https://docs.newrelic.com/"
configurations:
  api-key: "New Relic license key, must be an 'INGEST - LICENSE' key (see https://one.newrelic.com/api-keys)"
defaultExportURL: "otlp.nr-data.net:443"
allowCustomExportURL: true
presetScripts:
  - name: "HTTP Metrics"
    description: "This script sends HTTP metrics to New Relic's OTel endpoint."
    script: |
      #px:set max_output_rows_per_table=10000

      import px

      df = px.DataFrame(table='http_events', start_time=px.plugin.start_time, end_time=px.plugin.end_time)

      ns_prefix = df.ctx['namespace'] + '/'
      df.container = df.ctx['container_name']
      df.pod = px.strip_prefix(ns_prefix, df.ctx['pod'])
      df.service = px.strip_prefix(ns_prefix, df.ctx['service'])
      df.namespace = df.ctx['namespace']

      df.status_code = df.resp_status

      df = df.groupby(['status_code', 'pod', 'container','service', 'namespace']).agg(
          latency_min=('latency', px.min),
          latency_max=('latency', px.max),
          latency_sum=('latency', px.sum),
          latency_count=('latency', px.count),
          time_=('time_', px.max),
      )

      df.latency_min = df.latency_min / 1000000
      df.latency_max = df.latency_max / 1000000
      df.latency_sum = df.latency_sum / 1000000

      df.cluster_name = px.vizier_name()
      df.cluster_id = px.vizier_id()
      df.pixie = 'pixie'

      px.export(
        df, px.otel.Data(
          resource={
            'service.name': df.service,
            'k8s.container.name': df.container,
            'service.instance.id': df.pod,
            'k8s.pod.name': df.pod,
            'k8s.namespace.name': df.namespace,
            'px.cluster.id': df.cluster_id,
            'k8s.cluster.name': df.cluster_name,
            'instrumentation.provider': df.pixie,
          },
          data=[
            px.otel.metric.Summary(
              name='http.server.duration',
              description='measures the duration of the inbound HTTP request',
              # Unit is not supported yet
              # unit='ms',
              count=df.latency_count,
              sum=df.latency_sum,
              quantile_values={
                0.0: df.latency_min,
                1.0: df.latency_max,
              },
              attributes={
                'http.status_code': df.status_code,
              },
          )],
        ),
      )
    defaultFrequencyS: 10
  - name: "HTTP Spans"
    description: "This script sends HTTP span events (distributed tracing) to New Relic's OTel endpoint."
    script: |
      #px:set max_output_rows_per_table=1500

      import px

      df = px.DataFrame('http_events', start_time=px.plugin.start_time, end_time=px.plugin.end_time)

      ns_prefix = df.ctx['namespace'] + '/'
      df.container = df.ctx['container_name']
      df.pod = px.strip_prefix(ns_prefix, df.ctx['pod'])
      df.service = px.strip_prefix(ns_prefix, df.ctx['service'])
      df.namespace = df.ctx['namespace']

      df = df.head(15000)
      df.req_start_time = df.time_ - df.latency
      df.parent_pod_id = px.ip_to_pod_id(df.remote_addr)
      df.parent_service = px.replace('.*/', px.pod_id_to_service_name(df.parent_pod_id), '')
      df.parent_pod = px.replace('.*/', px.pod_id_to_pod_name(df.parent_pod_id), '')
      df.host = px.pluck(df.req_headers, 'Host')
      df.req_url = df.host + df.req_path

      df.user_agent = px.pluck(df.req_headers, 'User-Agent')
      df.trace_id = px.pluck(df.req_headers, 'X-B3-TraceId')
      df.span_id = px.pluck(df.req_headers, 'X-B3-SpanId')
      df.parent_span_id = px.pluck(df.req_headers, 'X-B3-ParentSpanId')

      # Strip out all but the actual path value from req_path
      df.req_path = px.uri_recompose('', '', '', 0, px.pluck(px.uri_parse(df.req_path), 'path'), '', '')

      # Replace any Hex IDS from the path with <id>
      df.req_path = px.replace('/[a-fA-F0-9\-:]{6,}(/?)', df.req_path, '/<id>\\1')

      df.cluster_name = px.vizier_name()
      df.cluster_id = px.vizier_id()
      df.pixie = 'pixie'

      px.export(
        df, px.otel.Data(
          resource={
            'service.name': df.service,
            'k8s.container.name': df.container,
            'service.instance.id': df.pod,
            'k8s.pod.name': df.pod,
            'k8s.namespace.name': df.namespace,
            'pixie.cluster.id': df.cluster_id,
            'k8s.cluster.name': df.cluster_name,
            'instrumentation.provider': df.pixie,
          },
          data=[
            px.otel.trace.Span(
              name=df.req_path,
              start_time=df.req_start_time,
              end_time=df.time_,
              trace_id=df.trace_id,
              span_id=df.span_id,
              parent_span_id=df.parent_span_id,
              kind=px.otel.trace.SPAN_KIND_SERVER,
              attributes={
                # NOTE: the integration handles splitting of services.
                'parent.service.name': df.parent_service,
                'parent.k8s.pod.name': df.parent_pod,
                'http.method': df.req_method,
                'http.url': df.req_url,
                'http.target': df.req_path,
                'http.host': df.host,
                'http.status_code': df.resp_status,
                'http.user_agent': df.user_agent,
              },
            ),
          ],
        ),
      )
    defaultFrequencyS: 10
  - name: "JVM Metrics"
    description: "This script sends JVM metrics to New Relic's OTel endpoint."
    script: |
      #px:set max_output_rows_per_table=10000

      import px

      df = px.DataFrame('jvm_stats', start_time=px.plugin.start_time, end_time=px.plugin.end_time)

      ns_prefix = df.ctx['namespace'] + '/'
      df.container = df.ctx['container_name']
      df.pod = px.strip_prefix(ns_prefix, df.ctx['pod'])
      df.service = px.strip_prefix(ns_prefix, df.ctx['service'])
      df.namespace = df.ctx['namespace']

      df.used_heap_size = px.Bytes(df.used_heap_size)
      df.total_heap_size = px.Bytes(df.total_heap_size)
      df.max_heap_size = px.Bytes(df.max_heap_size)

      # Aggregate over each process, k8s_object.
      df = df.groupby(['upid','container', 'pod', 'service', 'namespace']).agg(
          young_gc_time_max=('young_gc_time', px.max),
          young_gc_time_min=('young_gc_time', px.min),
          full_gc_time_max=('full_gc_time', px.max),
          full_gc_time_min=('full_gc_time', px.min),
          used_heap_size=('used_heap_size', px.mean),
          total_heap_size=('total_heap_size', px.mean),
          max_heap_size=('max_heap_size', px.mean),
          time_=('time_', px.max),
      )

      # Convert the counter metrics into accumulated values over the window.
      df.young_gc_time = df.young_gc_time_max - df.young_gc_time_min
      df.full_gc_time = df.full_gc_time_max - df.full_gc_time_min

      # Aggregate over each k8s_object.
      df = df.groupby(['container', 'pod', 'service', 'namespace']).agg(
          young_gc_time=('young_gc_time', px.sum),
          full_gc_time=('full_gc_time', px.sum),
          used_heap_size=('used_heap_size', px.sum),
          max_heap_size=('max_heap_size', px.sum),
          total_heap_size=('total_heap_size', px.sum),
          time_=('time_', px.max),
      )
      df.young_gc_time = px.DurationNanos(df.young_gc_time) / 1000000.0
      df.full_gc_time = px.DurationNanos(df.full_gc_time) / 1000000.0

      df.cluster_name = px.vizier_name()
      df.cluster_id = px.vizier_id()
      df.pixie = 'pixie'

      df.young = 'young'
      df.full = 'full'
      df.used = 'used'
      df.total = 'total'
      df.heap = 'area'
      df.max = 'max'

      px.export(
        df, px.otel.Data(
          resource={
            'service.name': df.service,
            'k8s.container.name': df.container,
            'service.instance.id': df.pod,
            'k8s.pod.name': df.pod,
            'k8s.namespace.name': df.namespace,
            'px.cluster.id': df.cluster_id,
            'k8s.cluster.name': df.cluster_name,
            'instrumentation.provider': df.pixie,
          },
          data=[
            px.otel.metric.Gauge(
              name='runtime.jvm.gc.collection',
              description='',
              # Unit is not supported yet
              # unit='ms',
              value=df.young_gc_time,
              attributes={'gc': df.young},
            ),
            px.otel.metric.Gauge(
              name='runtime.jvm.gc.collection',
              description=''
              # Unit is not supported yet
              # unit='ms',
              value=df.full_gc_time,
              attributes={'gc': df.full},
            ),
            px.otel.metric.Gauge(
              name='runtime.memory.area',
              description=''
              # Unit is not supported yet
              # unit='bytes',
              value=df.used_heap_size,
              attributes={'type': df.used, 'area': df.heap},
            ),
            px.otel.metric.Gauge(
              name='runtime.memory.area',
              description=''
              # Unit is not supported yet
              # unit='bytes',
              value=df.total_heap_size,
              attributes={'type': df.total, 'area': df.heap},
            ),
            px.otel.metric.Gauge(
              name='runtime.memory.area',
              description=''
              # Unit is not supported yet
              # unit='bytes',
              value=df.max_heap_size,
              attributes={'type': df.max, 'area': df.heap},
            ),
          ],
        ),
      )
    defaultFrequencyS: 10
  - name: "PostgreSQL Spans"
    description: "This script generates span events from queries to PostgreSQL databases and send them to New Relic's OTel endpoint."
    script: |
      #px:set max_output_rows_per_table=500

      import px

      df = px.DataFrame('pgsql_events', start_time=px.plugin.start_time, end_time=px.plugin.end_time)

      ns_prefix = df.ctx['namespace'] + '/'
      df.container = df.ctx['container_name']
      df.pod = px.strip_prefix(ns_prefix, df.ctx['pod'])
      df.service = px.strip_prefix(ns_prefix, df.ctx['service'])
      df.namespace = df.ctx['namespace']

      df.normed_query_struct = px.normalize_pgsql(df.req, df.req_cmd)
      df.query = px.pluck(df.normed_query_struct, 'query')

      df = df[df.query != ""]
      df = df[df.trace_role == 1]

      df.start_time = df.time_
      df.end_time = df.time_ + df.latency

      df = df[['start_time', 'end_time', 'container', 'service', 'pod', 'namespace', 'query', 'latency']]

      df.cluster_name = px.vizier_name()
      df.cluster_id = px.vizier_id()
      df.pixie = 'pixie'
      df.db_system = 'postgres'

      px.export(
        df, px.otel.Data(
          resource={
            'service.name': df.service,
            'k8s.container.name': df.container,
            'service.instance.id': df.pod,
            'k8s.pod.name': df.pod,
            'k8s.namespace.name': df.namespace,
            'pixie.cluster.id': df.cluster_id,
            'k8s.cluster.name': df.cluster_name,
            'instrumentation.provider': df.pixie,
          },
          data=[
            px.otel.trace.Span(
              name=df.query,
              start_time=df.start_time,
              end_time=df.end_time,
              kind=px.otel.trace.SPAN_KIND_CLIENT,
              attributes={
                'db.system': df.db_system,
              },
            ),
          ],
        ),
      )
    defaultFrequencyS: 10
  - name: "MySQL Spans"
    description: "This script generates span events from queries to MySQL databases and send them to New Relic's OTel endpoint."
    script: |
      #px:set max_output_rows_per_table=500

      import px

      df = px.DataFrame('mysql_events', start_time=px.plugin.start_time, end_time=px.plugin.end_time)

      ns_prefix = df.ctx['namespace'] + '/'
      df.container = df.ctx['container_name']
      df.pod = px.strip_prefix(ns_prefix, df.ctx['pod'])
      df.service = px.strip_prefix(ns_prefix, df.ctx['service'])
      df.namespace = df.ctx['namespace']

      df.normed_query_struct = px.normalize_mysql(df.req_body, df.req_cmd)
      df.query = px.pluck(df.normed_query_struct, 'query')

      df = df[df.query != ""]
      df = df[df.trace_role == 1]

      df.start_time = df.time_
      df.end_time = df.time_ + df.latency

      df = df[['start_time', 'end_time', 'container', 'service', 'pod', 'namespace', 'query', 'latency']]

      df.cluster_name = px.vizier_name()
      df.cluster_id = px.vizier_id()
      df.pixie = 'pixie'
      df.db_system = 'mysql'

      px.export(
        df, px.otel.Data(
          resource={
            'service.name': df.service,
            'k8s.container.name': df.container,
            'service.instance.id': df.pod,
            'k8s.pod.name': df.pod,
            'k8s.namespace.name': df.namespace,
            'pixie.cluster.id': df.cluster_id,
            'k8s.cluster.name': df.cluster_name,
            'instrumentation.provider': df.pixie,
          },
          data=[
            px.otel.trace.Span(
              name=df.query,
              start_time=df.start_time,
              end_time=df.end_time,
              kind=px.otel.trace.SPAN_KIND_CLIENT,
              attributes={
                'db.system': df.db_system,
              },
            ),
          ],
        ),
      )
    defaultFrequencyS: 10
  - name: "Kafka Metrics"
    description: "This script generates metrics from Kafka and sends them to New Relic's OTel endpoint."
    defaultDisabled: true
    defaultFrequencyS: 10
    script: |
      import px

      def unnest_topics_and_partitions(df, body_field: str):
          '''
          Unnest the topics and partitions from a data frame. body_field is the target column to unnest,
          usually 'req_body' or 'resp'.
          '''
          # Get topic_name
          df.topics = px.pluck(df[body_field], 'topics')
          df = json_unnest_first5(df, 'topic', 'topics')
          df = df[df.topic != '']
          df.topic_name = px.pluck(df.topic, 'name')

          # Get partition_idx
          df.partitions = px.pluck(df.topic, 'partitions')
          df = json_unnest_first5(df, 'partition', 'partitions')
          df = df[df.partition != '']
          df.partition_idx = px.pluck(df.partition, 'index')

          # Get message_size
          df.message_set = px.pluck(df.partition, 'message_set')
          df.message_size = px.pluck(df.message_set, 'size')
          df.message_size = px.atoi(df.message_size, 0)
          return df


      def json_unnest_first5(df, dest_col, src_col):
          '''Unnest the first 5 values in a JSON array in the src_col, and put it in the
          dest_col.
          '''
          df0 = json_array_index(df, dest_col, src_col, 0)
          df1 = json_array_index(df, dest_col, src_col, 1)
          df2 = json_array_index(df, dest_col, src_col, 2)
          df3 = json_array_index(df, dest_col, src_col, 3)
          df4 = json_array_index(df, dest_col, src_col, 4)
          df = df0.append(df1).append(df2).append(df3).append(df4)
          return df


      def json_array_index(df, dest_col, src_col, idx):
          df[dest_col] = px.pluck_array(df[src_col], idx)
          return df


      def select_columns(df):
          return df[[
              'time_', 'upid', 'req_cmd', 'client_id', 'latency', 'pod',
              'namespace', 'source', 'destination', 'service', 'topic_name',
              'partition_idx', 'message_size', 'error_code',
          ]]


      def get_produce_records(df):
          '''
          Get all the produce records and filter by a specified topic. If topic is empty, all
          produce records are retained.
          '''
          # Produce requests have command 0.
          producer_df = df[df.req_cmd == 0]

          producer_df = unnest_topics_and_partitions(producer_df, 'req_body')
          producer_df.req_partition_idx = df.partition_idx
          # Error code is always in the response.
          producer_df = unnest_topics_and_partitions(producer_df, 'resp')

          producer_df.error_code = px.pluck(producer_df.partition, 'error_code')
          producer_df = producer_df[producer_df.partition_idx == producer_df.req_partition_idx]
          return select_columns(producer_df)


      def get_fetch_records(df):
          '''
          Get all the fetch records and filter by a specified topic. If topic is empty, all
          fetch records are retained.
          '''
          # Fetch requests have command 1.
          consumer_df = df[df.req_cmd == 1]

          consumer_df = unnest_topics_and_partitions(consumer_df, 'resp')
          consumer_df.error_code = px.pluck(consumer_df.partition, 'error_code')
          return select_columns(consumer_df)


      def get_remaining_records(df):
          '''
          Get all the fetch records and filter by a specified topic. If topic is empty, all
          fetch records are retained.
          '''
          # Exclude Produce (cmd 0) and Fetch (cmd 1) commands.
          df = df[df.req_cmd > 1]
          df.topic_name = ''
          df.partition_idx = ''
          df.message_size = 0
          df.error_code = '0'
          return select_columns(df)


      def add_source_dest_columns(df):
          '''Add source and destination columns for the Kafka request.

          Kafka requests are traced server-side (trace_role==2), unless the server is
          outside of the cluster in which case the request is traced client-side (trace_role==1).

          When trace_role==2, the Kafka request source is the remote_addr column
          and destination is the pod column. When trace_role==1, the Kafka request
          source is the pod column and the destination is the remote_addr column.

          Input DataFrame must contain trace_role, upid, remote_addr columns.
          '''
          df.pod = df.ctx['pod']
          df.namespace = df.ctx['namespace']

          # If remote_addr is a pod, get its name. If not, use IP address.
          df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
          df.is_ra_pod = df.ra_pod != ''
          df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

          df.is_server_tracing = df.trace_role == 2
          df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
          df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

          # Set source and destination based on trace_role.
          df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
          df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

          # Filter out messages with empty source / destination.
          df = df[df.source != '']
          df = df[df.destination != '']
          df.kafka_service = px.Service(px.pod_name_to_service_name(df.destination))
          df.service = px.Service(px.pod_name_to_service_name(df.source))
          df.pod = df.source

          df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

          return df

      df = px.DataFrame(table='kafka_events.beta', start_time=px.plugin.start_time, end_time=px.plugin.end_time)
      df.namespace = df.ctx['namespace']

      df = add_source_dest_columns(df)
      producer_df = get_produce_records(df)
      consumer_df = get_fetch_records(df)
      remaining_df = get_remaining_records(df)
      df = producer_df.append([consumer_df, remaining_df])

      # Convert latency from ns units to ms units.
      df.latency = df.latency / (1000.0 * 1000.0)

      # Get throughput by adding size of message_sets. Note that this is the total size of the
      # message batches, not the total number of bytes sent or received.
      df.has_error = df.error_code != 'kNone' and df.error_code != '0'
      df = df.groupby(
          ['has_error', 'service', 'pod', 'partition_idx', 'client_id', 'topic_name', 'req_cmd', 'destination'],
      ).agg(
          record_bytes_total=('message_size', px.sum),
          latency_count=('message_size', px.count),
          latency_sum=('latency', px.sum),
          latency_max=('latency', px.max),
          latency_min=('latency', px.min),
          time_=('time_', px.max),
      )

      df.record_bytes_total = px.Bytes(df.record_bytes_total)
      df.namespace = px.pod_name_to_namespace(df.pod)
      df.pixie = 'pixie'
      df.cluster_id = px.vizier_id()
      df.cluster_name = px.vizier_name()
      df.req_cmd = px.kafka_api_key_name(df.req_cmd)

      otel_df = df
      px.export(
          otel_df,
          px.otel.Data(
              resource={
                  'service.name': otel_df.service,
                  'service.instance.id': otel_df.pod,
                  'k8s.pod.name': otel_df.pod,
                  'k8s.namespace.name': otel_df.namespace,
                  'k8s.cluster.name': otel_df.cluster_name,
                  'px.cluster.id': otel_df.cluster_id,
                  'instrumentation.provider': otel_df.pixie,
                  'kafka.broker.pod': otel_df.destination,
                  'kafka.partition_idx': otel_df.partition_idx,
                  'kafka.client_id': otel_df.client_id,
                  'kafka.topic_name': otel_df.topic_name,
                  'kafka.req_cmd': otel_df.req_cmd,
                  'kafka.has_error': otel_df.has_error,
              },
              data=[
                  px.otel.metric.Summary(
                      name='kafka.latency',
                      description='Summary of latency for Kafka requests',
                      count=otel_df.latency_count,
                      sum=otel_df.latency_sum,
                      unit='ms',
                      quantile_values={
                          0.0: otel_df.latency_min,
                          1.0: otel_df.latency_max,
                      },
                  ),
                  px.otel.metric.Gauge(
                      name='kafka.bytes',
                      description='The total number of bytes sent or received through Kafka requests',
                      value=otel_df.record_bytes_total,
                  ),
              ],
          ),
      )
  - name: "Kafka Spans"
    description: "This script samples Kafka Spans and sends them to New Relic's OTel endpoint."
    defaultDisabled: true
    defaultFrequencyS: 10
    script: |
      import px

      def unnest_topics_and_partitions(df, body_field: str):
          '''
          Unnest the topics and partitions from a data frame. body_field is the target column to unnest,
          usually 'req_body' or 'resp'.
          '''
          # Get topic_name
          df.topics = px.pluck(df[body_field], 'topics')
          df = json_unnest_first5(df, 'topic', 'topics')
          df = df[df.topic != '']
          df.topic_name = px.pluck(df.topic, 'name')

          # Get partition_idx
          df.partitions = px.pluck(df.topic, 'partitions')
          df = json_unnest_first5(df, 'partition', 'partitions')
          df = df[df.partition != '']
          df.partition_idx = px.pluck(df.partition, 'index')

          # Get message_size
          df.message_set = px.pluck(df.partition, 'message_set')
          df.message_size = px.pluck(df.message_set, 'size')
          df.message_size = px.atoi(df.message_size, 0)
          return df


      def json_unnest_first5(df, dest_col, src_col):
          '''Unnest the first 5 values in a JSON array in the src_col, and put it in the
          dest_col.
          '''
          df0 = json_array_index(df, dest_col, src_col, 0)
          df1 = json_array_index(df, dest_col, src_col, 1)
          df2 = json_array_index(df, dest_col, src_col, 2)
          df3 = json_array_index(df, dest_col, src_col, 3)
          df4 = json_array_index(df, dest_col, src_col, 4)
          df = df0.append(df1).append(df2).append(df3).append(df4)
          return df


      def json_array_index(df, dest_col, src_col, idx):
          df[dest_col] = px.pluck_array(df[src_col], idx)
          return df


      def select_columns(df):
          return df[[
              'calling_pod',
              'calling_service',
              'client_id',
              'destination',
              'kafka_pod',
              'kafka_service',
              'error_code',
              'latency',
              'message_size',
              'namespace',
              'partition_idx',
              'partition',
              'req_body',
              'req_cmd',
              'resp',
              'time_',
              'topic_name',
          ]]


      def get_produce_records(df):
          '''
          Get all the produce records and filter by a specified topic. If topic is empty, all
          produce records are retained.
          '''
          # Produce requests have command 0.
          producer_df = df[df.req_cmd == 0]

          producer_df = unnest_topics_and_partitions(producer_df, 'req_body')
          producer_df.req_partition_idx = df.partition_idx
          # Error code is always in the response.
          producer_df = unnest_topics_and_partitions(producer_df, 'resp')

          producer_df.error_code = px.pluck(producer_df.partition, 'error_code')
          producer_df = producer_df[producer_df.partition_idx == producer_df.req_partition_idx]
          return select_columns(producer_df)


      def get_fetch_records(df):
          '''
          Get all the fetch records and filter by a specified topic. If topic is empty, all
          fetch records are retained.
          '''
          # Fetch requests have command 1.
          consumer_df = df[df.req_cmd == 1]

          consumer_df = unnest_topics_and_partitions(consumer_df, 'resp')
          consumer_df.error_code = px.pluck(consumer_df.partition, 'error_code')
          return select_columns(consumer_df)


      def get_remaining_records(df):
          '''
          Get all the fetch records and filter by a specified topic. If topic is empty, all
          fetch records are retained.
          '''
          # Exclude Produce (cmd 0) and Fetch (cmd 1) commands.
          df = df[df.req_cmd > 1]
          df.topic_name = ''
          df.partition_idx = ''
          df.partition = ''
          df.message_size = 0
          df.error_code = '0'
          return select_columns(df)


      def add_source_dest_columns(df):
          '''Add source and destination columns for the Kafka request.

          Kafka requests are traced server-side (trace_role==2), unless the server is
          outside of the cluster in which case the request is traced client-side (trace_role==1).

          When trace_role==2, the Kafka request source is the remote_addr column
          and destination is the pod column. When trace_role==1, the Kafka request
          source is the pod column and the destination is the remote_addr column.

          Input DataFrame must contain trace_role, upid, remote_addr columns.
          '''
          df.pod = df.ctx['pod']
          df.namespace = df.ctx['namespace']

          # If remote_addr is a pod, get its name. If not, use IP address.
          df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
          df.is_ra_pod = df.ra_pod != ''
          df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

          df.is_server_tracing = df.trace_role == 2
          df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
          df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

          # Set source and destination based on trace_role.
          df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
          df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

          # Filter out messages with empty source / destination.
          df = df[df.source != '']
          df = df[df.destination != '']
          df.kafka_service = px.Service(px.pod_name_to_service_name(df.destination))
          df.calling_service = px.Service(px.pod_name_to_service_name(df.source))
          df.calling_pod = df.source
          df.kafka_pod = df.destination

          df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

          return df

      df = px.DataFrame(table='kafka_events.beta', start_time=px.plugin.start_time, end_time=px.plugin.end_time)
      df.namespace = df.ctx['namespace']

      df = add_source_dest_columns(df)
      producer_df = get_produce_records(df)
      consumer_df = get_fetch_records(df)
      remaining_df = get_remaining_records(df)
      df = producer_df.append([consumer_df, remaining_df])

      # Convert latency from ns units to ms units.
      df.latency_ns = df.latency
      df.latency = df.latency / (1000.0 * 1000.0)

      # Get throughput by adding size of message_sets. Note that this is the total size of the
      # message batches, not the total number of bytes sent or received.
      df.has_error = df.error_code != 'kNone' and df.error_code != '0'

      df.kafka_namespace = px.pod_name_to_namespace(df.kafka_pod)

      # Restrict number of results.
      df = df.head(1500)


      df.start_time = df.time_ - df.latency_ns
      df.req_cmd = px.kafka_api_key_name(df.req_cmd)
      df.pixie = "pixie"
      df.cluster_id = px.vizier_id()
      df.cluster_name = px.vizier_name()
      df.span_name = df.req_cmd + '/' + df.topic_name

      px.export(
          df,
          px.otel.Data(
              resource={
                  "service.name": df.kafka_service,
                  "service.instance.id": df.kafka_pod,
                  "k8s.pod.name": df.kafka_pod,
                  'k8s.namespace.name': df.kafka_namespace,
                  "k8s.cluster.name": df.cluster_name,
                  "instrumentation.provider": df.pixie,
                  "pixie.cluster.id": df.cluster_id,
                  'instrumentation.provider': df.pixie,
              },
              data=[
                  px.otel.trace.Span(
                      name=df.span_name,
                      start_time=df.start_time,
                      end_time=df.time_,
                      kind=px.otel.trace.SPAN_KIND_SERVER,
                      attributes={
                          "kafka.client_id": df.client_id,
                          "kafka.has_error": df.has_error,
                          "kafka.message_size": df.message_size,
                          "kafka.partition_idx": df.partition_idx,
                          "kafka.partition": df.partition,
                          "kafka.req_body": df.req_body,
                          "kafka.req_cmd": df.req_cmd,
                          "kafka.resp": df.resp,
                          "kafka.topic": df.topic_name,
                          "parent.k8s.pod.name": df.calling_pod,
                          "parent.service.name": df.calling_service,
                      },
                  )
              ],
          ),
      )

